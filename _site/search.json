[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is the Lab Journal of Lucas Rousselange for the CDSfBA course in the WiSe2024.\n\n[Status]: [ 70%]"
  },
  {
    "objectID": "content/01_journal/09_iv.html",
    "href": "content/01_journal/09_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/07_matching.html",
    "href": "content/01_journal/07_matching.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "Code\n#Load data set and libraries\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(RColorBrewer)\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(jtools)\nlibrary(kableExtra)\nlibrary(MatchIt)\nmembership &lt;- readRDS(\"../../../Causal_Data_Science_Data/membership.rds\")"
  },
  {
    "objectID": "content/01_journal/07_matching.html#exploring-the-relationships",
    "href": "content/01_journal/07_matching.html#exploring-the-relationships",
    "title": "Matching and Subclassification",
    "section": "Exploring the relationships",
    "text": "Exploring the relationships\nPlotting a heat map for the available factors, there seems to be a high correlation between age, average purchase and pre membership average purchase.The pre membership average purchase, age and membership card also seem correlated. The sex seems to not have a big influence on either variable.\n\n\nCode\nmem_cor &lt;- as.data.frame(cor(membership))\nmem_heat &lt;- mem_cor %&gt;%\n  rownames_to_column() %&gt;%\n  gather(colname, value, -rowname)\n\nggplot(mem_heat, aes(x=colname,y=rowname, fill=value)) + \n  geom_tile(color='black')+\n  scale_fill_gradient(low = \"white\", high = 'blue4')+\n  geom_text(aes(label = round(value,2)), color = \"black\", size = 4) +\n  labs(x='', y='', title = 'Heatmap correlation between variables')"
  },
  {
    "objectID": "content/01_journal/07_matching.html#dag",
    "href": "content/01_journal/07_matching.html#dag",
    "title": "Matching and Subclassification",
    "section": "DAG",
    "text": "DAG\n\n\nCode\n# Confounder\nconfounding &lt;- dagify(\n  X ~ Z1,\n  Y ~ Z1,\n  Y ~ X,\n  X ~ Z2,\n  Y ~ Z2,\n  Z2 ~ Z1,\n  coords = list(x = c(Y = 3, Z1 = 1, X = 1, Z2=3),\n                y = c(Y = 0, Z1 = 1, X = 0, Z2 =1)),\n  labels = list(Z2 = \"pre_avg_purch\",\n                Y = \"avg_purch\",\n                Z1 = \"age\",\n                X= 'card')\n  )\n\n\np &lt;- ggdag(confounding) +\n  geom_dag_point(color = \"#15c1d4\") +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"black\") +\n  geom_dag_label_repel(aes(label = label))+\n  theme_dag_blank()\np"
  },
  {
    "objectID": "content/01_journal/07_matching.html#coarsened-exact-matching",
    "href": "content/01_journal/07_matching.html#coarsened-exact-matching",
    "title": "Matching and Subclassification",
    "section": "(Coarsened) Exact Matching:",
    "text": "(Coarsened) Exact Matching:\n\n\nCode\ncem &lt;- matchit(card ~ age + pre_avg_purch,\n               data = membership, \n               method = 'cem', \n               estimand = 'ATE')\nsummary(cem)\n\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + pre_avg_purch, data = membership, \n#&gt;     method = \"cem\", estimand = \"ATE\")\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2136     1.1524    0.0438\n#&gt; pre_avg_purch       76.3938       66.0438          0.3962     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 40.2869       40.2645          0.0017     1.0001    0.0016\n#&gt; pre_avg_purch       70.5402       70.1875          0.0135     0.9910    0.0044\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0067          0.1224\n#&gt; pre_avg_purch   0.0133          0.1557\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.   4232.  \n#&gt; Matched (ESS) 5527.11 3928.04\n#&gt; Matched       5752.   4199.  \n#&gt; Unmatched       16.     33.  \n#&gt; Discarded        0.      0.\n\n\nCode\ndf_cem &lt;- match.data(cem)\nlm_cem &lt;- lm(avg_purch ~ card, data = df_cem, weights = weights)\nsumm(lm_cem)\n\n\n\n\n\n\nObservations\n9951\n\n\nDependent variable\navg_purch\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\nF(1,9949)\n616.09\n\n\nR²\n0.06\n\n\nAdj. R²\n0.06\n\n\n\n\n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n70.05\n0.40\n175.30\n0.00\n\n\ncard\n15.27\n0.62\n24.82\n0.00\n\n\n\n Standard errors: OLS"
  },
  {
    "objectID": "content/01_journal/07_matching.html#nearest-neighbor-matching",
    "href": "content/01_journal/07_matching.html#nearest-neighbor-matching",
    "title": "Matching and Subclassification",
    "section": "Nearest-Neighbor Matching:",
    "text": "Nearest-Neighbor Matching:\n\n\nCode\nnn &lt;- matchit(card ~ age + pre_avg_purch,\n               data = membership, \n               method = 'cem', \n               estimand = 'ATE')\nsummary(nn)\n\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + pre_avg_purch, data = membership, \n#&gt;     method = \"cem\", estimand = \"ATE\")\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2136     1.1524    0.0438\n#&gt; pre_avg_purch       76.3938       66.0438          0.3962     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 40.2869       40.2645          0.0017     1.0001    0.0016\n#&gt; pre_avg_purch       70.5402       70.1875          0.0135     0.9910    0.0044\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0067          0.1224\n#&gt; pre_avg_purch   0.0133          0.1557\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.   4232.  \n#&gt; Matched (ESS) 5527.11 3928.04\n#&gt; Matched       5752.   4199.  \n#&gt; Unmatched       16.     33.  \n#&gt; Discarded        0.      0.\n\n\nCode\ndf_nn &lt;- match.data(nn)\nlm_nn &lt;- lm(avg_purch ~ card, data = df_nn, weights = weights)\nsumm(lm_nn)\n\n\n\n\n\n\nObservations\n9951\n\n\nDependent variable\navg_purch\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\nF(1,9949)\n616.09\n\n\nR²\n0.06\n\n\nAdj. R²\n0.06\n\n\n\n\n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n70.05\n0.40\n175.30\n0.00\n\n\ncard\n15.27\n0.62\n24.82\n0.00\n\n\n\n Standard errors: OLS"
  },
  {
    "objectID": "content/01_journal/07_matching.html#inverse-probability-weighting",
    "href": "content/01_journal/07_matching.html#inverse-probability-weighting",
    "title": "Matching and Subclassification",
    "section": "Inverse Probability Weighting:",
    "text": "Inverse Probability Weighting:\n\n\nCode\n# (1) Propensity scores\nmodel_prop &lt;- glm(card ~ age + pre_avg_purch,\n                  data = membership,\n                  family = binomial(link = \"logit\"))\nsummary(model_prop)\n\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = card ~ age + pre_avg_purch, family = binomial(link = \"logit\"), \n#&gt;     data = membership)\n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)   -1.4121735  0.0723729 -19.512   &lt;2e-16 ***\n#&gt; age            0.0011734  0.0017758   0.661    0.509    \n#&gt; pre_avg_purch  0.0148181  0.0009263  15.996   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 13626  on 9999  degrees of freedom\n#&gt; Residual deviance: 13250  on 9997  degrees of freedom\n#&gt; AIC: 13256\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n\nCode\n# Add propensities to table\nmembership_aug &lt;- membership %&gt;% mutate(propensity = predict(model_prop, type = \"response\"))\n\n# Extend data by IPW scores\nmembership_ipw &lt;- membership_aug %&gt;% mutate(\n  ipw = (card/propensity) + ((1-card) / (1-propensity)))\n\n# Run with high weights excluded\nlm_ipw &lt;- lm(avg_purch ~ card,\n                data = membership_ipw %&gt;% filter(propensity %&gt;% between(0.15, 0.85)),\n                weights = ipw)\nsumm(lm_ipw)\n\n\n\n\n\n\nObservations\n10000\n\n\nDependent variable\navg_purch\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\nF(1,9998)\n599.18\n\n\nR²\n0.06\n\n\nAdj. R²\n0.06\n\n\n\n\n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n70.26\n0.43\n162.65\n0.00\n\n\ncard\n14.95\n0.61\n24.48\n0.00\n\n\n\n Standard errors: OLS"
  },
  {
    "objectID": "content/01_journal/07_matching.html#summary",
    "href": "content/01_journal/07_matching.html#summary",
    "title": "Matching and Subclassification",
    "section": "Summary",
    "text": "Summary\n\n\nCode\nmodelsummary::modelsummary(list(\"Naive\" = lm_naive,\n                                \"CEM1\"  = lm_cem,\n                                \"NN\"    = lm_nn,\n                                \"IPW1\"  = lm_ipw))\n\n\n\n\n\n\nNaive\nCEM1\nNN\n IPW1\n\n\n\n\n(Intercept)\n65.940\n70.047\n70.047\n70.263\n\n\n\n(0.397)\n(0.400)\n(0.400)\n(0.432)\n\n\ncard\n25.220\n15.269\n15.269\n14.955\n\n\n\n(0.610)\n(0.615)\n(0.615)\n(0.611)\n\n\nNum.Obs.\n10000\n9951\n9951\n10000\n\n\nR2\n0.146\n0.058\n0.058\n0.057\n\n\nR2 Adj.\n0.146\n0.058\n0.058\n0.056\n\n\nAIC\n96483.2\n96352.4\n96352.4\n97072.9\n\n\nBIC\n96504.8\n96374.0\n96374.0\n97094.5\n\n\nLog.Lik.\n−48238.590\n−48173.187\n−48173.187\n−48533.447\n\n\nRMSE\n30.11\n30.27\n30.27\n30.54"
  },
  {
    "objectID": "content/01_journal/05_dag.html",
    "href": "content/01_journal/05_dag.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "The DAG for the store parking spot example only contains two nodes:\n\n\\(D\\): Parking spots (Treatment variable)\n\\(Y\\): Sales (Outcome variable)\n\n\n\nCode\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(ggplot2)\ndag_model &lt;- 'dag {\nD [exposure,pos=\"0.075,0.4\"]\nY [outcome,pos=\"0.4,0.4\"]\nD -&gt; Y\n}'\n# draw DAG\np &lt;- ggdag_status(dag_model) +\n  guides(fill = \"none\", color = \"none\") +  # Disable the legend\n  geom_dag_edges(edge_color = \"black\")+\n  theme_dag_blank()\np"
  },
  {
    "objectID": "content/01_journal/05_dag.html#comparison",
    "href": "content/01_journal/05_dag.html#comparison",
    "title": "Directed Acyclic Graphs",
    "section": "Comparison",
    "text": "Comparison\nComparing both models, we can see that our model which accounts for subscriptions has a much better accuracy than the one that does not.A coefficient reversal takes place when changing models. For our follow-up only model, the follow-up correlation coefficient is \\(R =-3.31\\), while it changes to \\(R =2.19\\) when subscrition model is taken into account.\n\nFollow-up only\n\n\nCode\neffect_plot(lm_sat, follow_ups, interval = TRUE, plot.points = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nFollow-up & Subscrition\n\n\nCode\neffect_plot(lm_sat_mod, follow_ups, interval = TRUE, plot.points = TRUE)"
  },
  {
    "objectID": "content/01_journal/05_dag.html#explanation",
    "href": "content/01_journal/05_dag.html#explanation",
    "title": "Directed Acyclic Graphs",
    "section": "Explanation",
    "text": "Explanation\nA possible explanation is that the subscription is a confounding factor. Another possible explanation is that the subscription is a collision factor. The most probable explanation is that the subscription is a confounding factor, as the follow-up calls rise and the satisfaction falls with increasing subscription level. Users who use the software the most and the most features (Elite) encounter more problems and need more follow-up calls. This in turn reduces their satisfaction.\n\n\nCode\n# Confounder\nconfounding &lt;- dagify(\n  X ~ Z,\n  Y ~ Z,\n  Y ~ X,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 0, Z = 1, X = 0)),\n  labels = list(X = \"follow-ups\",\n                Y = \"satisfaction\",\n                Z = \"subscription\")\n  )\n\n# Plot DAG\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(ggplot2)\np &lt;- ggdag(confounding) +\n  geom_dag_point(color = \"#15c1d4\") +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"black\") +\n  geom_dag_label_repel(aes(label = label))+\n  theme_dag_blank()\np\n\n\n\n\n\n\n\n\n\n\nEffect of subscription category on satisfaction\n\n\nCode\neffect_plot(lm_sat_mod, subscription, interval = TRUE, plot.points = TRUE , \n            jitter = 0.1)\n\n\n\n\n\n\n\n\n\n\n\nEffect of subscription category on follow-up\n\n\nCode\nlm_sub&lt;-lm(follow_ups ~ subscription, data=customer_sat)\neffect_plot(lm_sub, subscription, interval = TRUE, plot.points = TRUE , \n            jitter = 0.1)"
  },
  {
    "objectID": "content/01_journal/05_dag.html#not-conditioned",
    "href": "content/01_journal/05_dag.html#not-conditioned",
    "title": "Directed Acyclic Graphs",
    "section": "Not conditioned",
    "text": "Not conditioned\n\n\nCode\nsimps_not_cond"
  },
  {
    "objectID": "content/01_journal/05_dag.html#conditioned",
    "href": "content/01_journal/05_dag.html#conditioned",
    "title": "Directed Acyclic Graphs",
    "section": "Conditioned",
    "text": "Conditioned\n\n\nCode\nsimps_cond"
  },
  {
    "objectID": "content/01_journal/03_regression.html",
    "href": "content/01_journal/03_regression.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "Task 1\n\n\nCode\ncar_prices &lt;- readRDS(\"../../../Causal_Data_Science_Data/car_prices.rds\")\n\n\nUsing the Environement pane in RStudio, we can see that the data has 181 rows and 22 columns.\n\n\nTask 2\n\n\nCode\nlibrary(pillar)\nglimpse(car_prices,width=79)\n\n\n#&gt; Rows: 181\n#&gt; Columns: 22\n#&gt; $ aspiration       &lt;chr&gt; \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"st…\n#&gt; $ doornumber       &lt;chr&gt; \"two\", \"two\", \"two\", \"four\", \"four\", \"two\", \"four\", …\n#&gt; $ carbody          &lt;chr&gt; \"convertible\", \"convertible\", \"hatchback\", \"sedan\", …\n#&gt; $ drivewheel       &lt;chr&gt; \"rwd\", \"rwd\", \"rwd\", \"fwd\", \"4wd\", \"fwd\", \"fwd\", \"fw…\n#&gt; $ enginelocation   &lt;chr&gt; \"front\", \"front\", \"front\", \"front\", \"front\", \"front\"…\n#&gt; $ wheelbase        &lt;dbl&gt; 88.6, 88.6, 94.5, 99.8, 99.4, 99.8, 105.8, 105.8, 10…\n#&gt; $ carlength        &lt;dbl&gt; 168.8, 168.8, 171.2, 176.6, 176.6, 177.3, 192.7, 192…\n#&gt; $ carwidth         &lt;dbl&gt; 64.1, 64.1, 65.5, 66.2, 66.4, 66.3, 71.4, 71.4, 71.4…\n#&gt; $ carheight        &lt;dbl&gt; 48.8, 48.8, 52.4, 54.3, 54.3, 53.1, 55.7, 55.7, 55.9…\n#&gt; $ curbweight       &lt;dbl&gt; 2548, 2548, 2823, 2337, 2824, 2507, 2844, 2954, 3086…\n#&gt; $ enginetype       &lt;chr&gt; \"dohc\", \"dohc\", \"ohcv\", \"ohc\", \"ohc\", \"ohc\", \"ohc\", …\n#&gt; $ cylindernumber   &lt;chr&gt; \"four\", \"four\", \"six\", \"four\", \"five\", \"five\", \"five…\n#&gt; $ enginesize       &lt;dbl&gt; 130, 130, 152, 109, 136, 136, 136, 136, 131, 131, 10…\n#&gt; $ fuelsystem       &lt;chr&gt; \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpf…\n#&gt; $ boreratio        &lt;dbl&gt; 3.47, 3.47, 2.68, 3.19, 3.19, 3.19, 3.19, 3.19, 3.13…\n#&gt; $ stroke           &lt;dbl&gt; 2.68, 2.68, 3.47, 3.40, 3.40, 3.40, 3.40, 3.40, 3.40…\n#&gt; $ compressionratio &lt;dbl&gt; 9.00, 9.00, 9.00, 10.00, 8.00, 8.50, 8.50, 8.50, 8.3…\n#&gt; $ horsepower       &lt;dbl&gt; 111, 111, 154, 102, 115, 110, 110, 110, 140, 160, 10…\n#&gt; $ peakrpm          &lt;dbl&gt; 5000, 5000, 5000, 5500, 5500, 5500, 5500, 5500, 5500…\n#&gt; $ citympg          &lt;dbl&gt; 21, 21, 19, 24, 18, 19, 19, 19, 17, 16, 23, 23, 21, …\n#&gt; $ highwaympg       &lt;dbl&gt; 27, 27, 26, 30, 22, 25, 25, 25, 20, 22, 29, 29, 28, …\n#&gt; $ price            &lt;dbl&gt; 13495.00, 16500.00, 16500.00, 13950.00, 17450.00, 15…\n\n\nUsing the glimpse() function form the Pillar package, we can see that all the strings are of type &lt;chr&gt;, while all numeric values are of type &lt;dbl&gt;, which means double precision floating point.\n\n\nTask 3\nLooking at the summary of the linear regression model with the following command\n\n\nCode\nlm_all &lt;- lm(price ~ ., data = car_prices)\nsummary(lm_all)\n\n\nwe can see that the variable shown below have a significant impact on the car price:\n\nStroke\nPeak RPM\nEngine Size\nCar width\nCar body (hardtop, hatchback, wagon)\nEngine location (rear)\nEngine type (ohc, ohcv)\nCylinder number (four, five, six, twelve)\n\nIncluding all the above cited significant variables via the following command:\n\n\nCode\nlm &lt;- lm(price ~ stroke + peakrpm + enginesize + carwidth\n          + carbody + enginelocation + enginetype  + cylindernumber,\n         data = car_prices)\nsummary(lm)\n\n\n\n\nCode\nlibrary(jtools)\nlm_all &lt;- lm(price ~ ., data = car_prices)\nlm &lt;- lm(price ~ stroke + peakrpm + enginesize + carwidth\n          + carbody + enginelocation + enginetype  + cylindernumber,\n         data = car_prices)\n\nlm_R2 &lt;- summary(lm)$adj.r.squared\nlm_R2 &lt;- sprintf(\"%0.2f\",lm_R2)\nlm_all_R2 &lt;- summary(lm_all)$adj.r.squared\nlm_all_R2 &lt;- sprintf(\"%0.2f\",lm_all_R2)\n\n\nwe get an adjusted \\(R^2 = 0.92\\). The previous adjusted \\(R^2\\) of our model containing all variables was \\(R^2 = 0.93\\). We lost a bit in prediction accuracy but also reduced the model complexity significantly. A detailed comparison is shwn below:\n\n\nCode\nexport_summs(lm,lm_all, scale = TRUE)\n\n\n\n\n\nModel 1Model 2\n\n(Intercept)22748.16 ***24079.02 ***\n\n(2449.85)   (3525.93)   \n\nstroke-1656.67 ***-1457.62 ***\n\n(252.78)   (297.09)   \n\npeakrpm1092.60 ***1067.31 ***\n\n(211.27)   (267.96)   \n\nenginesize6260.61 ***5315.59 ***\n\n(610.89)   (1120.25)   \n\ncarwidth1809.82 ***1534.23 ** \n\n(389.31)   (512.65)   \n\ncarbodyhardtop-4378.37 ** -3691.74 *  \n\n(1342.93)   (1424.82)   \n\ncarbodyhatchback-3425.43 ** -3344.33 ** \n\n(1131.92)   (1238.36)   \n\ncarbodysedan-2648.73 *  -2292.82    \n\n(1129.89)   (1356.01)   \n\ncarbodywagon-3302.84 ** -3427.92 *  \n\n(1219.12)   (1490.28)   \n\nenginelocationrear7106.52 ** 6643.49 *  \n\n(2194.86)   (2572.28)   \n\nenginetypedohcv-6919.56 *  -8541.96    \n\n(3100.12)   (4749.69)   \n\nenginetypel1650.40    978.75    \n\n(1404.98)   (1786.38)   \n\nenginetypeohc3038.48 ***3345.25 ***\n\n(864.71)   (933.00)   \n\nenginetypeohcf-32.65    972.92    \n\n(1226.78)   (1625.63)   \n\nenginetypeohcv-6151.77 ***-6222.32 ***\n\n(1157.39)   (1236.42)   \n\ncylindernumberfive-8868.08 ***-11724.54 ***\n\n(2475.69)   (3019.19)   \n\ncylindernumberfour-9556.60 ***-11549.33 ***\n\n(2317.61)   (3177.18)   \n\ncylindernumbersix-5203.95 ** -7151.40 ** \n\n(1705.90)   (2247.23)   \n\ncylindernumberthree-2129.44    -4318.93    \n\n(3791.45)   (4688.83)   \n\ncylindernumbertwelve-13583.04 ***-11122.21 ** \n\n(2927.53)   (4196.49)   \n\naspirationturbo       1846.21    \n\n       (1041.39)   \n\ndoornumbertwo       242.52    \n\n       (571.93)   \n\ndrivewheelfwd       -504.56    \n\n       (1076.62)   \n\ndrivewheelrwd       -15.45    \n\n       (1268.07)   \n\nwheelbase       -170.88    \n\n       (525.02)   \n\ncarlength       -364.75    \n\n       (633.75)   \n\ncarheight       291.26    \n\n       (318.24)   \n\ncurbweight       1322.17    \n\n       (901.58)   \n\nfuelsystem2bbl       177.14    \n\n       (883.62)   \n\nfuelsystemmfi       -3041.02    \n\n       (2577.00)   \n\nfuelsystemmpfi       359.28    \n\n       (1001.53)   \n\nfuelsystemspdi       -2543.89    \n\n       (1363.55)   \n\nfuelsystemspfi       514.77    \n\n       (2499.23)   \n\nboreratio       -356.18    \n\n       (447.63)   \n\ncompressionratio       -511.45    \n\n       (385.34)   \n\nhorsepower       417.93    \n\n       (922.01)   \n\ncitympg       -566.32    \n\n       (1044.53)   \n\nhighwaympg       1012.00    \n\n       (1092.32)   \n\nN181       181       \n\nR20.93    0.94    \n\nAll continuous predictors are mean-centered and scaled by 1 standard deviation. The outcome variable is in its original units.  *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\n\n\nTask 4\nChosen regressor: enginesize\n\nThe engine size is of type &lt;dbl&gt; and can take on values between 61 and 326\nThe engine size has a positive influence on the price, i.e. a higher engine size, is correlated with a higher car price.\nThe effect is significant (\\(p&lt;0.001\\))\n\n\n\nCode\nep &lt;- effect_plot(lm, pred = enginesize, interval = TRUE, plot.points = TRUE, \n            jitter = 0.05)  \nep + theme_apa()\n\n\n\n\n\n\n\n\n\n\n\nTask 5\nAs we can see below, the OLS method is not able to compute a linear regression coefficient for our newline define variable. This is due to the fact that all out entries for this variable are the same, which results in variance that is exactly equal to \\(0\\). The OLS method uses the variance of the independent variable as denominator and is therefore not able to determine a corresponding linear regression.\n\n\nCode\nlibrary(dplyr)\ncar_sh &lt;-  car_prices %&gt;% mutate(seat_heating  = TRUE)\nsh_lm &lt;- lm(price ~ seat_heating, data = car_sh)\nsumm(sh_lm)\n\n\n\n\n\n\nObservations\n181\n\n\nDependent variable\nprice\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n12999.35\n599.70\n21.68\n0.00\n\n\nseat_heatingTRUE\nNA\nNA\nNA\nNA\n\n\n\n Standard errors: OLS"
  },
  {
    "objectID": "content/01_journal/01_probability.html",
    "href": "content/01_journal/01_probability.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Code\nP_S &lt;- 0.3 \nP_nS &lt;- 1-P_S\nP_ST &lt;- 0.2 \nP_SnT &lt;- 1-P_ST\nP_nST &lt;- 0.6\nP_nSnT &lt;- 1-P_nST \n\nP_S_T &lt;- P_S*P_ST\nP_nS_T &lt;- P_nS*P_nST\nP_S_nT &lt;- P_S*P_SnT\nP_nS_nT &lt;- P_nS*P_nSnT\n\n\n\n\n\n\\(P(S) = 0.3\\)\n\n\\(P(T \\mid S) = 0.2\\)\n\\(P(\\overline{T} \\mid S) = 0.8\\)\n\n\\(P(\\overline{S}) = 0.7\\)\n\n\\(P(T \\mid \\overline{S}) = 0.6\\)\n\\(P(\\overline{T} \\mid \\overline{S}) = 0.4\\)\n\n\n\n\n\n\n\n\\[\n\\begin{split}\nP(T \\cap S) = P(S) * P(T \\mid S) =0.3*0.2=0.06,\\\\\nP(T \\cap \\overline{S}) = P(\\overline{S}) * P(T \\mid \\overline{S}) =0.7*0.6=0.42,\\\\\nP(\\overline{T} \\cap S) = P(S) * P(\\overline{T} \\mid S) =0.3*0.8=0.24,\\\\\nP(\\overline{T} \\cap \\overline{S}) = P(\\overline{S}) * P(\\overline{T} \\mid \\overline{S}) =0.7*0.4=0.28.\n\\end{split}\n\\]\n\n\n\n\\[\nP(T \\cap S ) + P(T \\cap \\overline{S}) + P(\\overline{T} \\cap S) + P(\\overline{T} \\cap \\overline{S})\n= 1\n\\]"
  },
  {
    "objectID": "content/01_journal/01_probability.html#given-probabilities",
    "href": "content/01_journal/01_probability.html#given-probabilities",
    "title": "Probability Theory",
    "section": "",
    "text": "\\(P(S) = 0.3\\)\n\n\\(P(T \\mid S) = 0.2\\)\n\\(P(\\overline{T} \\mid S) = 0.8\\)\n\n\\(P(\\overline{S}) = 0.7\\)\n\n\\(P(T \\mid \\overline{S}) = 0.6\\)\n\\(P(\\overline{T} \\mid \\overline{S}) = 0.4\\)"
  },
  {
    "objectID": "content/01_journal/01_probability.html#answer",
    "href": "content/01_journal/01_probability.html#answer",
    "title": "Probability Theory",
    "section": "",
    "text": "\\[\n\\begin{split}\nP(T \\cap S) = P(S) * P(T \\mid S) =0.3*0.2=0.06,\\\\\nP(T \\cap \\overline{S}) = P(\\overline{S}) * P(T \\mid \\overline{S}) =0.7*0.6=0.42,\\\\\nP(\\overline{T} \\cap S) = P(S) * P(\\overline{T} \\mid S) =0.3*0.8=0.24,\\\\\nP(\\overline{T} \\cap \\overline{S}) = P(\\overline{S}) * P(\\overline{T} \\mid \\overline{S}) =0.7*0.4=0.28.\n\\end{split}\n\\]\n\n\n\n\\[\nP(T \\cap S ) + P(T \\cap \\overline{S}) + P(\\overline{T} \\cap S) + P(\\overline{T} \\cap \\overline{S})\n= 1\n\\]"
  },
  {
    "objectID": "content/01_journal/02_statistics.html",
    "href": "content/01_journal/02_statistics.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Task 1\n\n\nCode\noptions(scipen = 999)\nrandom_vars &lt;- readRDS(\"../../../Causal_Data_Science_Data/random_vars.rds\")\nage &lt;- random_vars$age\ninc &lt;- random_vars$income\nage_m &lt;- mean(age)\ninc_m &lt;- mean(inc)\nage_v &lt;- var(age)\ninc_v &lt;- var(inc)\nage_sd &lt;- sd(age)\ninc_sd &lt;- sd(inc)\nrd &lt;- 1\n\n\n\n\n\nVariable\nAge\nIncome\n\n\n\n\nExpected value\n33.5\n3510.7\n\n\nVariance\n340.6\n8625645.8\n\n\nStandard deviation\n18.5\n2936.9\n\n\n\n\n\nTask 2\n\n\nCode\n#age_sd/age_m*100\n#inc_sd/inc_m*100\n\n\nDepending on the goal it might make sense to compare both standard deviation relative to their mean, also known as the Coefficient of Variation (CV). This is possible as both variables are absolute and it might inform us of possible data analysis steps we can take. But, in general, it does not really make sense to compare the standard deviation of both distributions directly as they do not have the same unit, i.e. they represent completely different variables.\n\n\nTask 3\n\n\nCode\ncov_age_inc &lt;- cov(age,inc)\ncor_age_inc &lt;- cor(age,inc)\n\n\n\nCovariance: 29700.1\nCorrelation: 0.55\n\n\n\nTask 4\nThe pearson correlation coefficient is much easier to interpret, as it is a dimensionless value that has a predefined interval \\([-1,1]\\). This makes it possible to compare the strength of different relationships via the correlation coefficient.\nNote: The pearson correlation coefficient is sensitive to outlier and is best used with the assumption of normality. Other correlations, such as Spearman or Kendall, are more robust when the data is not normaly distributed.\n\n\nTask 5\n\n\nCode\nlibrary(dplyr)\nunder_age &lt;- filter(random_vars,age &gt; 0 & age &lt;= 18)\nactive &lt;- filter(random_vars, age &gt; 18 & age &lt;=65)\nretired &lt;- filter(random_vars, age &gt; 65)\n\n\n\n\n\nAge group\n[0,18]\n[18,65[\n[65,\\(\\infty\\)[\n\n\n\n\n\\(E[income]\\)\n391.22\n4743.23\n1604.19"
  },
  {
    "objectID": "content/01_journal/04_causality.html",
    "href": "content/01_journal/04_causality.html",
    "title": "Causality",
    "section": "",
    "text": "Below you can see the correlation between the import value of tomatoes in Germany and the number of cosmetic surgeries world wide.\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(jtools)\nAB &lt;- read.csv(\"../../../Causal_Data_Science_Data/AB.csv\")\nscale &lt;- 0.55\ny_str2 &lt;-'Tomato import in Germany [100k€]'\ny_str1 &lt;- 'Cosmetic surgeries world wide'\np &lt;- ggplot(AB, aes(x=Jahr, y=OP)) +\n  geom_line(aes(color = y_str1)) +\n  geom_line(aes(y = Wert/scale,  color = y_str2)) +\n  scale_y_continuous(sec.axis = sec_axis(~.*scale, name=y_str2)) +\n  labs(x = \"Year\", y = y_str1 )\np + theme_apa() + theme(legend.position=\"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nfit &lt;- lm(AB$OP ~ AB$Wert)\nfit_R2 &lt;- cor(AB$OP, AB$Wert)\nfit_R2 &lt;- sprintf(\"%0.2f\",fit_R2)\n\n\nAs you can imagine their is no causal link between the two. Nerveless, there is a significant correlation between both variables. The Pearson correlation coefficient is \\(R = 0.80\\). Both samples are normaly distributed."
  },
  {
    "objectID": "content/01_journal/04_causality.html#cosmetic-surgeries-and-tomato-import-value",
    "href": "content/01_journal/04_causality.html#cosmetic-surgeries-and-tomato-import-value",
    "title": "Causality",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(jtools)\nAB &lt;- read.csv(\"../../../Causal_Data_Science_Data/AB.csv\")\nscale &lt;- 0.55\ny_str2 &lt;-'Tomato import in Germany [100k€]'\ny_str1 &lt;- 'Cosmetic surgeries world wide'\np &lt;- ggplot(AB, aes(x=Jahr, y=OP)) +\n  geom_line(aes(color = y_str1)) +\n  geom_line(aes(y = Wert/scale,  color = y_str2)) +\n  scale_y_continuous(sec.axis = sec_axis(~.*scale, name=y_str2)) +\n  labs(x = \"Year\", y = y_str1 )\np + theme_apa() + theme(legend.position=\"bottom\")"
  },
  {
    "objectID": "content/01_journal/04_causality.html#correlation",
    "href": "content/01_journal/04_causality.html#correlation",
    "title": "Causality",
    "section": "",
    "text": "Code\nfit &lt;- lm(AB$OP ~ AB$Wert)\nfit_R2 &lt;- cor(AB$OP, AB$Wert)\nfit_R2 &lt;- sprintf(\"%0.2f\",fit_R2)\n\n\nAs you can imagine their is no causal link between the two. Nerveless, there is a significant correlation between both variables. The Pearson correlation coefficient is \\(R = 0.80\\). Both samples are normaly distributed."
  },
  {
    "objectID": "content/01_journal/06_rct.html",
    "href": "content/01_journal/06_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "Code\n#Load data set and libraries\nlibrary(ggplot2)\nlibrary(jtools)\nlibrary(kableExtra)\nabtest_online &lt;- readRDS(\"../../../Causal_Data_Science_Data/abtest_online.rds\")"
  },
  {
    "objectID": "content/01_journal/06_rct.html#previous-site-visits",
    "href": "content/01_journal/06_rct.html#previous-site-visits",
    "title": "Randomized Controlled Trials",
    "section": "Previous site visits",
    "text": "Previous site visits\nAs shown below, we can see that both groups look pretty similar, i.e. balanced with respect to previous site visits.\n\n\nCode\ncompare_visits &lt;- \n  ggplot(abtest_online, \n         aes(x = chatbot, \n             y = previous_visit, \n             color = as.factor(chatbot))) +\n  geom_boxplot(width=0.3, fill=\"white\")+\n  stat_boxplot(geom = 'errorbar', width = 0.4)+\n  labs(x = \"Chatbot acitve\", y = \"Previous visits\", title = 'Previous site visit balance')+\n  theme_minimal()+\n  theme(legend.position = \"none\")\ncompare_visits"
  },
  {
    "objectID": "content/01_journal/06_rct.html#mobile-device-usage",
    "href": "content/01_journal/06_rct.html#mobile-device-usage",
    "title": "Randomized Controlled Trials",
    "section": "Mobile device usage",
    "text": "Mobile device usage\nAs the mobile device usage variable is a boolean, we can only compare the ratio of mobile device across both groups. Again, both groups seem balanced.\n\n\nCode\n#Calculate mobile device usage for each group\nmbu_aggregate=aggregate(abtest_online$mobile_device,by=list(abtest_online$chatbot),FUN=mean)\nnames(mbu_aggregate)&lt;-c('chatbot', 'mobile_device')\n\ncompare_device &lt;- \n  ggplot(mbu_aggregate, \n         aes(x = chatbot, \n             y = mobile_device, \n             color = as.factor(chatbot))) +\n  geom_boxplot(width=0.3, fill=\"white\")+\n  labs(x = \"Chatbot acitve\", y = \"Previous visits\", title = 'Mobile device usage balance')+\n  ylim(0,1)+\n  theme_minimal()+\n  theme(legend.position = \"none\")\ncompare_device"
  },
  {
    "objectID": "content/01_journal/08_did.html",
    "href": "content/01_journal/08_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "Code\n#Load data set and libraries\nlibrary(ggplot2)\nlibrary(jtools)\nlibrary(kableExtra)\nlibrary(dplyr)\nhospdd &lt;- readRDS(\"../../../Causal_Data_Science_Data/hospdd.rds\")\n\n\n\nTask 1\n\n\nCode\n# Before treatment\nctrl_satis_before &lt;- hospdd %&gt;%\n  filter(month &lt;4, procedure == 0) %&gt;% \n  pull(satis)\nctrl_satis_before &lt;- mean(ctrl_satis_before)\ncsb &lt;- sprintf(\"%0.2f\",ctrl_satis_before)\n\ntrmt_satis_before &lt;- hospdd %&gt;%\n  filter(month &lt;4, procedure == 0) %&gt;% \n  pull(satis)\ntrmt_satis_before &lt;- mean(trmt_satis_before)\ntsb &lt;- sprintf(\"%0.2f\",trmt_satis_before)\n\n\n\n# After treatment\nctrl_satis_after &lt;- hospdd %&gt;%\n  filter(month &gt;=4, procedure == 0) %&gt;% \n  pull(satis)\nctrl_satis_after &lt;- mean(ctrl_satis_after)\ncsa &lt;- sprintf(\"%0.2f\",ctrl_satis_after)\n\n\ntrmt_satis_after &lt;- hospdd %&gt;%\n  filter(month &gt;=4, procedure == 0) %&gt;% \n  pull(satis)\ntrmt_satis_after &lt;- mean(trmt_satis_after)\ntsa &lt;- sprintf(\"%0.2f\",trmt_satis_after)\n\n\nThe satisfaction in the groups were as follows:\n\n\n\nGroup\nCtrl - Before\nTrmt - Before\nCtrl - After\nTrmt - After\n\n\n\n\nSatisfaction\n3.45\n3.45\n3.38\n3.38\n\n\n\n\n\nTask 2\nwe want to include month and hospital as variable and not as factor to be able to see an overall trend.\n\n\nCode\nsumm(lm(satis ~ procedure*(month + hospital), data = hospdd))\n\n\n\n\n\n\nObservations\n7368\n\n\nDependent variable\nsatis\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\nF(5,7362)\n227.64\n\n\nR²\n0.13\n\n\nAdj. R²\n0.13\n\n\n\n\n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n3.56\n0.03\n111.68\n0.00\n\n\nprocedure\n0.64\n0.14\n4.68\n0.00\n\n\nmonth\n-0.00\n0.01\n-0.68\n0.50\n\n\nhospital\n-0.00\n0.00\n-4.15\n0.00\n\n\nprocedure:month\n0.01\n0.02\n0.53\n0.60\n\n\nprocedure:hospital\n0.02\n0.01\n3.46\n0.00\n\n\n\n Standard errors: OLS"
  },
  {
    "objectID": "content/01_journal/10_rdd.html",
    "href": "content/01_journal/10_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  }
]