[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is the Lab Journal of Lucas Rousselange for the CDSfBA course in the WiSe2024.\n\n[Status]: [ 50%]"
  },
  {
    "objectID": "content/01_journal/09_iv.html",
    "href": "content/01_journal/09_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/07_matching.html",
    "href": "content/01_journal/07_matching.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/05_dag.html",
    "href": "content/01_journal/05_dag.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "The DAG for the store parking spot example only contains two nodes:\n\n\\(D\\): Parking spots (Treatment variable)\n\\(Y\\): Sales (Outcome variable)"
  },
  {
    "objectID": "content/01_journal/05_dag.html#comparison",
    "href": "content/01_journal/05_dag.html#comparison",
    "title": "Directed Acyclic Graphs",
    "section": "Comparison",
    "text": "Comparison\nComparing both models, we can see that our model which accounts for subscriptions has a much better accuracy than the one that does not.A coefficient reversal takes place when changing models. For our follow-up only model, the follow-up correlation coefficient is \\(R =-3.31\\), while it changes to \\(R =2.19\\) when subscrition model is taken into account.\n\nFollow-up only\n\n\n\n\n\n\n\n\n\n\n\nFollow-up & Subscrition"
  },
  {
    "objectID": "content/01_journal/05_dag.html#explanation",
    "href": "content/01_journal/05_dag.html#explanation",
    "title": "Directed Acyclic Graphs",
    "section": "Explanation",
    "text": "Explanation\nA possible explanation is that the subscription is a confounding factor. Another possible explanation is that the subscription is a collision factor. The most probable explanation is that the subscription is a confounding factor, as the follow-up calls rise and the satisfaction falls with increasing subscription level. Users who use the software the most and the most features (Elite) encounter more problems and need more follow-up calls. This in turn reduces their satisfaction.\n\n\n\n\n\n\n\n\n\n\nEffect of subscription category on satisfaction\n\n\n\n\n\n\n\n\n\n\n\nEffect of subscription category on follow-up"
  },
  {
    "objectID": "content/01_journal/05_dag.html#not-conditioned",
    "href": "content/01_journal/05_dag.html#not-conditioned",
    "title": "Directed Acyclic Graphs",
    "section": "Not conditioned",
    "text": "Not conditioned"
  },
  {
    "objectID": "content/01_journal/05_dag.html#conditioned",
    "href": "content/01_journal/05_dag.html#conditioned",
    "title": "Directed Acyclic Graphs",
    "section": "Conditioned",
    "text": "Conditioned"
  },
  {
    "objectID": "content/01_journal/03_regression.html",
    "href": "content/01_journal/03_regression.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "Task 1\nUsing the Environement pane in RStudio, we can see that the data has 181 rows and 22 columns.\n\n\nTask 2\n\n\n#&gt; Rows: 181\n#&gt; Columns: 22\n#&gt; $ aspiration       &lt;chr&gt; \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"st…\n#&gt; $ doornumber       &lt;chr&gt; \"two\", \"two\", \"two\", \"four\", \"four\", \"two\", \"four\", …\n#&gt; $ carbody          &lt;chr&gt; \"convertible\", \"convertible\", \"hatchback\", \"sedan\", …\n#&gt; $ drivewheel       &lt;chr&gt; \"rwd\", \"rwd\", \"rwd\", \"fwd\", \"4wd\", \"fwd\", \"fwd\", \"fw…\n#&gt; $ enginelocation   &lt;chr&gt; \"front\", \"front\", \"front\", \"front\", \"front\", \"front\"…\n#&gt; $ wheelbase        &lt;dbl&gt; 88.6, 88.6, 94.5, 99.8, 99.4, 99.8, 105.8, 105.8, 10…\n#&gt; $ carlength        &lt;dbl&gt; 168.8, 168.8, 171.2, 176.6, 176.6, 177.3, 192.7, 192…\n#&gt; $ carwidth         &lt;dbl&gt; 64.1, 64.1, 65.5, 66.2, 66.4, 66.3, 71.4, 71.4, 71.4…\n#&gt; $ carheight        &lt;dbl&gt; 48.8, 48.8, 52.4, 54.3, 54.3, 53.1, 55.7, 55.7, 55.9…\n#&gt; $ curbweight       &lt;dbl&gt; 2548, 2548, 2823, 2337, 2824, 2507, 2844, 2954, 3086…\n#&gt; $ enginetype       &lt;chr&gt; \"dohc\", \"dohc\", \"ohcv\", \"ohc\", \"ohc\", \"ohc\", \"ohc\", …\n#&gt; $ cylindernumber   &lt;chr&gt; \"four\", \"four\", \"six\", \"four\", \"five\", \"five\", \"five…\n#&gt; $ enginesize       &lt;dbl&gt; 130, 130, 152, 109, 136, 136, 136, 136, 131, 131, 10…\n#&gt; $ fuelsystem       &lt;chr&gt; \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpf…\n#&gt; $ boreratio        &lt;dbl&gt; 3.47, 3.47, 2.68, 3.19, 3.19, 3.19, 3.19, 3.19, 3.13…\n#&gt; $ stroke           &lt;dbl&gt; 2.68, 2.68, 3.47, 3.40, 3.40, 3.40, 3.40, 3.40, 3.40…\n#&gt; $ compressionratio &lt;dbl&gt; 9.00, 9.00, 9.00, 10.00, 8.00, 8.50, 8.50, 8.50, 8.3…\n#&gt; $ horsepower       &lt;dbl&gt; 111, 111, 154, 102, 115, 110, 110, 110, 140, 160, 10…\n#&gt; $ peakrpm          &lt;dbl&gt; 5000, 5000, 5000, 5500, 5500, 5500, 5500, 5500, 5500…\n#&gt; $ citympg          &lt;dbl&gt; 21, 21, 19, 24, 18, 19, 19, 19, 17, 16, 23, 23, 21, …\n#&gt; $ highwaympg       &lt;dbl&gt; 27, 27, 26, 30, 22, 25, 25, 25, 20, 22, 29, 29, 28, …\n#&gt; $ price            &lt;dbl&gt; 13495.00, 16500.00, 16500.00, 13950.00, 17450.00, 15…\n\n\nUsing the glimpse() function form the Pillar package, we can see that all the strings are of type &lt;chr&gt;, while all numeric values are of type &lt;dbl&gt;, which means double precision floating point.\n\n\nTask 3\nLooking at the summary of the linear regression model with the following command\n\nlm_all &lt;- lm(price ~ ., data = car_prices)\nsummary(lm_all)\n\nwe can see that the variable shown below have a significant impact on the car price:\n\nStroke\nPeak RPM\nEngine Size\nCar width\nCar body (hardtop, hatchback, wagon)\nEngine location (rear)\nEngine type (ohc, ohcv)\nCylinder number (four, five, six, twelve)\n\nIncluding all the above cited significant variables via the following command:\n\nlm &lt;- lm(price ~ stroke + peakrpm + enginesize + carwidth\n          + carbody + enginelocation + enginetype  + cylindernumber,\n         data = car_prices)\nsummary(lm)\n\nwe get an adjusted \\(R^2 = 0.92\\). The previous adjusted \\(R^2\\) of our model containing all variables was \\(R^2 = 0.93\\). We lost a bit in prediction accuracy but also reduced the model complexity significantly. A detailed comparison is shwn below:\n\n\n\n\n\nModel 1Model 2\n\n(Intercept)22748.16 ***24079.02 ***\n\n(2449.85)   (3525.93)   \n\nstroke-1656.67 ***-1457.62 ***\n\n(252.78)   (297.09)   \n\npeakrpm1092.60 ***1067.31 ***\n\n(211.27)   (267.96)   \n\nenginesize6260.61 ***5315.59 ***\n\n(610.89)   (1120.25)   \n\ncarwidth1809.82 ***1534.23 ** \n\n(389.31)   (512.65)   \n\ncarbodyhardtop-4378.37 ** -3691.74 *  \n\n(1342.93)   (1424.82)   \n\ncarbodyhatchback-3425.43 ** -3344.33 ** \n\n(1131.92)   (1238.36)   \n\ncarbodysedan-2648.73 *  -2292.82    \n\n(1129.89)   (1356.01)   \n\ncarbodywagon-3302.84 ** -3427.92 *  \n\n(1219.12)   (1490.28)   \n\nenginelocationrear7106.52 ** 6643.49 *  \n\n(2194.86)   (2572.28)   \n\nenginetypedohcv-6919.56 *  -8541.96    \n\n(3100.12)   (4749.69)   \n\nenginetypel1650.40    978.75    \n\n(1404.98)   (1786.38)   \n\nenginetypeohc3038.48 ***3345.25 ***\n\n(864.71)   (933.00)   \n\nenginetypeohcf-32.65    972.92    \n\n(1226.78)   (1625.63)   \n\nenginetypeohcv-6151.77 ***-6222.32 ***\n\n(1157.39)   (1236.42)   \n\ncylindernumberfive-8868.08 ***-11724.54 ***\n\n(2475.69)   (3019.19)   \n\ncylindernumberfour-9556.60 ***-11549.33 ***\n\n(2317.61)   (3177.18)   \n\ncylindernumbersix-5203.95 ** -7151.40 ** \n\n(1705.90)   (2247.23)   \n\ncylindernumberthree-2129.44    -4318.93    \n\n(3791.45)   (4688.83)   \n\ncylindernumbertwelve-13583.04 ***-11122.21 ** \n\n(2927.53)   (4196.49)   \n\naspirationturbo       1846.21    \n\n       (1041.39)   \n\ndoornumbertwo       242.52    \n\n       (571.93)   \n\ndrivewheelfwd       -504.56    \n\n       (1076.62)   \n\ndrivewheelrwd       -15.45    \n\n       (1268.07)   \n\nwheelbase       -170.88    \n\n       (525.02)   \n\ncarlength       -364.75    \n\n       (633.75)   \n\ncarheight       291.26    \n\n       (318.24)   \n\ncurbweight       1322.17    \n\n       (901.58)   \n\nfuelsystem2bbl       177.14    \n\n       (883.62)   \n\nfuelsystemmfi       -3041.02    \n\n       (2577.00)   \n\nfuelsystemmpfi       359.28    \n\n       (1001.53)   \n\nfuelsystemspdi       -2543.89    \n\n       (1363.55)   \n\nfuelsystemspfi       514.77    \n\n       (2499.23)   \n\nboreratio       -356.18    \n\n       (447.63)   \n\ncompressionratio       -511.45    \n\n       (385.34)   \n\nhorsepower       417.93    \n\n       (922.01)   \n\ncitympg       -566.32    \n\n       (1044.53)   \n\nhighwaympg       1012.00    \n\n       (1092.32)   \n\nN181       181       \n\nR20.93    0.94    \n\nAll continuous predictors are mean-centered and scaled by 1 standard deviation. The outcome variable is in its original units.  *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\n\n\nTask 4\nChosen regressor: enginesize\n\nThe engine size is of type &lt;dbl&gt; and can take on values between 61 and 326\nThe engine size has a positive influence on the price, i.e. a higher engine size, is correlated with a higher car price.\nThe effect is significant (\\(p&lt;0.001\\))\n\n\n\n\n\n\n\n\n\n\n\n\nTask 5\nAs we can see below, the OLS method is not able to compute a linear regression coefficient for our newline define variable. This is due to the fact that all out entries for this variable are the same, which results in variance that is exactly equal to \\(0\\). The OLS method uses the variance of the independent variable as denominator and is therefore not able to determine a corresponding linear regression.\n\n\n\n\n\n\nObservations\n181\n\n\nDependent variable\nprice\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n12999.35\n599.70\n21.68\n0.00\n\n\nseat_heatingTRUE\nNA\nNA\nNA\nNA\n\n\n\n Standard errors: OLS"
  },
  {
    "objectID": "content/01_journal/01_probability.html",
    "href": "content/01_journal/01_probability.html",
    "title": "Probability Theory",
    "section": "",
    "text": "\\(P(S) = 0.3\\)\n\n\\(P(T \\mid S) = 0.2\\)\n\\(P(\\overline{T} \\mid S) = 0.8\\)\n\n\\(P(\\overline{S}) = 0.7\\)\n\n\\(P(T \\mid \\overline{S}) = 0.6\\)\n\\(P(\\overline{T} \\mid \\overline{S}) = 0.4\\)\n\n\n\n\n\n\n\n\\[\n\\begin{split}\nP(T \\cap S) = P(S) * P(T \\mid S) =0.3*0.2=0.06,\\\\\nP(T \\cap \\overline{S}) = P(\\overline{S}) * P(T \\mid \\overline{S}) =0.7*0.6=0.42,\\\\\nP(\\overline{T} \\cap S) = P(S) * P(\\overline{T} \\mid S) =0.3*0.8=0.24,\\\\\nP(\\overline{T} \\cap \\overline{S}) = P(\\overline{S}) * P(\\overline{T} \\mid \\overline{S}) =0.7*0.4=0.28.\n\\end{split}\n\\]\n\n\n\n\\[\nP(T \\cap S ) + P(T \\cap \\overline{S}) + P(\\overline{T} \\cap S) + P(\\overline{T} \\cap \\overline{S})\n= 1\n\\]"
  },
  {
    "objectID": "content/01_journal/01_probability.html#given-probabilities",
    "href": "content/01_journal/01_probability.html#given-probabilities",
    "title": "Probability Theory",
    "section": "",
    "text": "\\(P(S) = 0.3\\)\n\n\\(P(T \\mid S) = 0.2\\)\n\\(P(\\overline{T} \\mid S) = 0.8\\)\n\n\\(P(\\overline{S}) = 0.7\\)\n\n\\(P(T \\mid \\overline{S}) = 0.6\\)\n\\(P(\\overline{T} \\mid \\overline{S}) = 0.4\\)"
  },
  {
    "objectID": "content/01_journal/01_probability.html#answer",
    "href": "content/01_journal/01_probability.html#answer",
    "title": "Probability Theory",
    "section": "",
    "text": "\\[\n\\begin{split}\nP(T \\cap S) = P(S) * P(T \\mid S) =0.3*0.2=0.06,\\\\\nP(T \\cap \\overline{S}) = P(\\overline{S}) * P(T \\mid \\overline{S}) =0.7*0.6=0.42,\\\\\nP(\\overline{T} \\cap S) = P(S) * P(\\overline{T} \\mid S) =0.3*0.8=0.24,\\\\\nP(\\overline{T} \\cap \\overline{S}) = P(\\overline{S}) * P(\\overline{T} \\mid \\overline{S}) =0.7*0.4=0.28.\n\\end{split}\n\\]\n\n\n\n\\[\nP(T \\cap S ) + P(T \\cap \\overline{S}) + P(\\overline{T} \\cap S) + P(\\overline{T} \\cap \\overline{S})\n= 1\n\\]"
  },
  {
    "objectID": "content/01_journal/02_statistics.html",
    "href": "content/01_journal/02_statistics.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Task 1\n\n\n\nVariable\nAge\nIncome\n\n\n\n\nExpected value\n33.5\n3510.7\n\n\nVariance\n340.6\n8.6256458^{6}\n\n\nStandard deviation\n18.5\n2936.9\n\n\n\n\n\nTask 2\nDepending on the goal it might make sense to compare both standard deviation relative to their mean, also known as the Coefficient of Variation (CV). This is possible as both variables are absolute and it might inform us of possible data analysis steps we can take. But, in general, it does not really make sense to compare the standard deviation of both distributions directly as they do not have the same unit, i.e. they represent completely different variables.\n\n\nTask 3\n\nCovariance: 2.97001^{4}\nCorrelation: 0.55\n\n\n\nTask 4\nThe pearson correlation coefficient is much easier to interpret, as it is a dimensionless value that has a predefined interval \\([-1,1]\\). This makes it possible to compare the strength of different relationships via the correlation coefficient.\nNote: The pearson correlation coefficient is sensitive to outlier and is best used with the assumption of normality. Other correlations, such as Spearman or Kendall, are more robust when the data is not normaly distributed.\n\n\nTask 5\n\n\n\nAge group\n[0,18]\n[18,65[\n[65,\\(\\infty\\)[\n\n\n\n\n\\(E[income]\\)\n391.22\n4743.23\n1604.19"
  },
  {
    "objectID": "content/01_journal/04_causality.html",
    "href": "content/01_journal/04_causality.html",
    "title": "Causality",
    "section": "",
    "text": "Below you can see the correlation between the import value of tomatoes in Germany and the number of cosmetic surgeries world wide.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you can imagine their is no causal link between the two. Nerveless, there is a significant correlation between both variables. The Pearson correlation coefficient is \\(R = 0.80\\). Both samples are normaly distributed."
  },
  {
    "objectID": "content/01_journal/04_causality.html#correlation",
    "href": "content/01_journal/04_causality.html#correlation",
    "title": "Causality",
    "section": "",
    "text": "As you can imagine their is no causal link between the two. Nerveless, there is a significant correlation between both variables. The Pearson correlation coefficient is \\(R = 0.80\\). Both samples are normaly distributed."
  },
  {
    "objectID": "content/01_journal/06_rct.html",
    "href": "content/01_journal/06_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "Code\n#Load data set and libraries\nlibrary(ggplot2)\nlibrary(jtools)\nlibrary(kableExtra)\nabtest_online &lt;- readRDS(\"../../../Causal_Data_Science_Data/abtest_online.rds\")"
  },
  {
    "objectID": "content/01_journal/08_did.html",
    "href": "content/01_journal/08_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/10_rdd.html",
    "href": "content/01_journal/10_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/06_rct.html#previous-site-visits",
    "href": "content/01_journal/06_rct.html#previous-site-visits",
    "title": "Randomized Controlled Trials",
    "section": "Previous site visits",
    "text": "Previous site visits\nAs shown below, we can see that both groups look pretty similar, i.e. balanced with respect to previous site visits.\n\n\nCode\ncompare_visits &lt;- \n  ggplot(abtest_online, \n         aes(x = chatbot, \n             y = previous_visit, \n             color = as.factor(chatbot))) +\n  geom_boxplot(width=0.3, fill=\"white\")+\n  stat_boxplot(geom = 'errorbar', width = 0.4)+\n  labs(x = \"Chatbot acitve\", y = \"Previous visits\", title = 'Previous site visit balance')+\n  theme_minimal()+\n  theme(legend.position = \"none\")\ncompare_visits"
  },
  {
    "objectID": "content/01_journal/06_rct.html#mobile-device-usage",
    "href": "content/01_journal/06_rct.html#mobile-device-usage",
    "title": "Randomized Controlled Trials",
    "section": "Mobile device usage",
    "text": "Mobile device usage\nAs the mobile device usage variable is a boolean, we can only compare the ratio of mobile device across both groups. Again, both groups seem balanced.\n\n\nCode\n#Calculate mobile device usage for each group\nmbu_aggregate=aggregate(abtest_online$mobile_device,by=list(abtest_online$chatbot),FUN=mean)\nnames(mbu_aggregate)&lt;-c('chatbot', 'mobile_device')\n\ncompare_device &lt;- \n  ggplot(mbu_aggregate, \n         aes(x = chatbot, \n             y = mobile_device, \n             color = as.factor(chatbot))) +\n  geom_boxplot(width=0.3, fill=\"white\")+\n  labs(x = \"Chatbot acitve\", y = \"Previous visits\", title = 'Mobile device usage balance')+\n  ylim(0,1)+\n  theme_minimal()+\n  theme(legend.position = \"none\")\ncompare_device"
  }
]