[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is the Lab Journal of Lucas Rousselange for the CDSfBA course in the WiSe2024.\n\n[Status]: [ 40%]"
  },
  {
    "objectID": "content/01_journal/09_iv.html",
    "href": "content/01_journal/09_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/07_matching.html",
    "href": "content/01_journal/07_matching.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/05_dag.html",
    "href": "content/01_journal/05_dag.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/03_regression.html",
    "href": "content/01_journal/03_regression.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "Task 1\nUsing the Environement pane in RStudio, we can see that the data has 181 rows and 22 columns.\n\n\nTask 2\n\n\n#&gt; Rows: 181\n#&gt; Columns: 22\n#&gt; $ aspiration       &lt;chr&gt; \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"st…\n#&gt; $ doornumber       &lt;chr&gt; \"two\", \"two\", \"two\", \"four\", \"four\", \"two\", \"four\", …\n#&gt; $ carbody          &lt;chr&gt; \"convertible\", \"convertible\", \"hatchback\", \"sedan\", …\n#&gt; $ drivewheel       &lt;chr&gt; \"rwd\", \"rwd\", \"rwd\", \"fwd\", \"4wd\", \"fwd\", \"fwd\", \"fw…\n#&gt; $ enginelocation   &lt;chr&gt; \"front\", \"front\", \"front\", \"front\", \"front\", \"front\"…\n#&gt; $ wheelbase        &lt;dbl&gt; 88.6, 88.6, 94.5, 99.8, 99.4, 99.8, 105.8, 105.8, 10…\n#&gt; $ carlength        &lt;dbl&gt; 168.8, 168.8, 171.2, 176.6, 176.6, 177.3, 192.7, 192…\n#&gt; $ carwidth         &lt;dbl&gt; 64.1, 64.1, 65.5, 66.2, 66.4, 66.3, 71.4, 71.4, 71.4…\n#&gt; $ carheight        &lt;dbl&gt; 48.8, 48.8, 52.4, 54.3, 54.3, 53.1, 55.7, 55.7, 55.9…\n#&gt; $ curbweight       &lt;dbl&gt; 2548, 2548, 2823, 2337, 2824, 2507, 2844, 2954, 3086…\n#&gt; $ enginetype       &lt;chr&gt; \"dohc\", \"dohc\", \"ohcv\", \"ohc\", \"ohc\", \"ohc\", \"ohc\", …\n#&gt; $ cylindernumber   &lt;chr&gt; \"four\", \"four\", \"six\", \"four\", \"five\", \"five\", \"five…\n#&gt; $ enginesize       &lt;dbl&gt; 130, 130, 152, 109, 136, 136, 136, 136, 131, 131, 10…\n#&gt; $ fuelsystem       &lt;chr&gt; \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpf…\n#&gt; $ boreratio        &lt;dbl&gt; 3.47, 3.47, 2.68, 3.19, 3.19, 3.19, 3.19, 3.19, 3.13…\n#&gt; $ stroke           &lt;dbl&gt; 2.68, 2.68, 3.47, 3.40, 3.40, 3.40, 3.40, 3.40, 3.40…\n#&gt; $ compressionratio &lt;dbl&gt; 9.00, 9.00, 9.00, 10.00, 8.00, 8.50, 8.50, 8.50, 8.3…\n#&gt; $ horsepower       &lt;dbl&gt; 111, 111, 154, 102, 115, 110, 110, 110, 140, 160, 10…\n#&gt; $ peakrpm          &lt;dbl&gt; 5000, 5000, 5000, 5500, 5500, 5500, 5500, 5500, 5500…\n#&gt; $ citympg          &lt;dbl&gt; 21, 21, 19, 24, 18, 19, 19, 19, 17, 16, 23, 23, 21, …\n#&gt; $ highwaympg       &lt;dbl&gt; 27, 27, 26, 30, 22, 25, 25, 25, 20, 22, 29, 29, 28, …\n#&gt; $ price            &lt;dbl&gt; 13495.00, 16500.00, 16500.00, 13950.00, 17450.00, 15…\n\n\nUsing the glimpse() function form the Pillar package, we can see that all the strings are of type &lt;chr&gt;, while all numeric values are of type &lt;dbl&gt;, which means double precision floating point.\n\n\nTask 3\nLooking at the summary of the linear regression model with the following command\n\nlm_all &lt;- lm(price ~ ., data = car_prices)\nsummary(lm_all)\n\nwe can see that the variable shown below have a significant impact on the car price:\n\nStroke\nPeak RPM\nEngine Size\nCar width\nCar body (hardtop, hatchback, wagon)\nEngine location (rear)\nEngine type (ohc, ohcv)\nCylinder number (four, five, six, twelve)\n\nIncluding all the above cited significant variables via the following command:\n\nlm &lt;- lm(price ~ stroke + peakrpm + enginesize + carwidth\n          + carbody + enginelocation + enginetype  + cylindernumber,\n         data = car_prices)\nsummary(lm)\n\nwe get an adjusted \\(R^2 = 0.92\\). The previous adjusted \\(R^2\\) of our model containing all variables was \\(R^2 = 0.93\\). We lost a bit in prediction accuracy but also reduced the model complexity significantly. A detailed comparison is shwn below:\n\n\n\n\n\nModel 1Model 2\n\n(Intercept)22748.16 ***24079.02 ***\n\n(2449.85)   (3525.93)   \n\nstroke-1656.67 ***-1457.62 ***\n\n(252.78)   (297.09)   \n\npeakrpm1092.60 ***1067.31 ***\n\n(211.27)   (267.96)   \n\nenginesize6260.61 ***5315.59 ***\n\n(610.89)   (1120.25)   \n\ncarwidth1809.82 ***1534.23 ** \n\n(389.31)   (512.65)   \n\ncarbodyhardtop-4378.37 ** -3691.74 *  \n\n(1342.93)   (1424.82)   \n\ncarbodyhatchback-3425.43 ** -3344.33 ** \n\n(1131.92)   (1238.36)   \n\ncarbodysedan-2648.73 *  -2292.82    \n\n(1129.89)   (1356.01)   \n\ncarbodywagon-3302.84 ** -3427.92 *  \n\n(1219.12)   (1490.28)   \n\nenginelocationrear7106.52 ** 6643.49 *  \n\n(2194.86)   (2572.28)   \n\nenginetypedohcv-6919.56 *  -8541.96    \n\n(3100.12)   (4749.69)   \n\nenginetypel1650.40    978.75    \n\n(1404.98)   (1786.38)   \n\nenginetypeohc3038.48 ***3345.25 ***\n\n(864.71)   (933.00)   \n\nenginetypeohcf-32.65    972.92    \n\n(1226.78)   (1625.63)   \n\nenginetypeohcv-6151.77 ***-6222.32 ***\n\n(1157.39)   (1236.42)   \n\ncylindernumberfive-8868.08 ***-11724.54 ***\n\n(2475.69)   (3019.19)   \n\ncylindernumberfour-9556.60 ***-11549.33 ***\n\n(2317.61)   (3177.18)   \n\ncylindernumbersix-5203.95 ** -7151.40 ** \n\n(1705.90)   (2247.23)   \n\ncylindernumberthree-2129.44    -4318.93    \n\n(3791.45)   (4688.83)   \n\ncylindernumbertwelve-13583.04 ***-11122.21 ** \n\n(2927.53)   (4196.49)   \n\naspirationturbo       1846.21    \n\n       (1041.39)   \n\ndoornumbertwo       242.52    \n\n       (571.93)   \n\ndrivewheelfwd       -504.56    \n\n       (1076.62)   \n\ndrivewheelrwd       -15.45    \n\n       (1268.07)   \n\nwheelbase       -170.88    \n\n       (525.02)   \n\ncarlength       -364.75    \n\n       (633.75)   \n\ncarheight       291.26    \n\n       (318.24)   \n\ncurbweight       1322.17    \n\n       (901.58)   \n\nfuelsystem2bbl       177.14    \n\n       (883.62)   \n\nfuelsystemmfi       -3041.02    \n\n       (2577.00)   \n\nfuelsystemmpfi       359.28    \n\n       (1001.53)   \n\nfuelsystemspdi       -2543.89    \n\n       (1363.55)   \n\nfuelsystemspfi       514.77    \n\n       (2499.23)   \n\nboreratio       -356.18    \n\n       (447.63)   \n\ncompressionratio       -511.45    \n\n       (385.34)   \n\nhorsepower       417.93    \n\n       (922.01)   \n\ncitympg       -566.32    \n\n       (1044.53)   \n\nhighwaympg       1012.00    \n\n       (1092.32)   \n\nN181       181       \n\nR20.93    0.94    \n\nAll continuous predictors are mean-centered and scaled by 1 standard deviation. The outcome variable is in its original units.  *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\n\n\nTask 4\nChosen regressor: enginesize\n\nThe engine size is of type &lt;dbl&gt; and can take on values between 61 and 326\nThe engine size has a positive influence on the price, i.e. a higher engine size, is correlated with a higher car price.\nThe effect is significant (\\(p&lt;0.001\\))\n\n\n\n\n\n\n\n\n\n\n\n\nTask 5\nAs we can see below, the OLS method is not able to compute a linear regression coefficient for our newline define variable. This is due to the fact that all out entries for this variable are the same, which results in variance that is exactly equal to \\(0\\). The OLS method uses the variance of the independent variable as denominator and is therefore not able to determine a corresponding linear regression.\n\n\n\n\n\n\nObservations\n181\n\n\nDependent variable\nprice\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n12999.35\n599.70\n21.68\n0.00\n\n\nseat_heatingTRUE\nNA\nNA\nNA\nNA\n\n\n\n Standard errors: OLS"
  },
  {
    "objectID": "content/01_journal/01_probability.html",
    "href": "content/01_journal/01_probability.html",
    "title": "Probability Theory",
    "section": "",
    "text": "\\(P(S) = 0.3\\)\n\n\\(P(T \\mid S) = 0.2\\)\n\\(P(\\overline{T} \\mid S) = 0.8\\)\n\n\\(P(\\overline{S}) = 0.7\\)\n\n\\(P(T \\mid \\overline{S}) = 0.6\\)\n\\(P(\\overline{T} \\mid \\overline{S}) = 0.4\\)\n\n\n\n\n\n\n\n\\[\n\\begin{split}\nP(T \\cap S) = P(S) * P(T \\mid S) =0.3*0.2=0.06,\\\\\nP(T \\cap \\overline{S}) = P(\\overline{S}) * P(T \\mid \\overline{S}) =0.7*0.6=0.42,\\\\\nP(\\overline{T} \\cap S) = P(S) * P(\\overline{T} \\mid S) =0.3*0.8=0.24,\\\\\nP(\\overline{T} \\cap \\overline{S}) = P(\\overline{S}) * P(\\overline{T} \\mid \\overline{S}) =0.7*0.4=0.28.\n\\end{split}\n\\]\n\n\n\n\\[\nP(T \\cap S ) + P(T \\cap \\overline{S}) + P(\\overline{T} \\cap S) + P(\\overline{T} \\cap \\overline{S})\n= 1\n\\]"
  },
  {
    "objectID": "content/01_journal/01_probability.html#given-probabilities",
    "href": "content/01_journal/01_probability.html#given-probabilities",
    "title": "Probability Theory",
    "section": "",
    "text": "\\(P(S) = 0.3\\)\n\n\\(P(T \\mid S) = 0.2\\)\n\\(P(\\overline{T} \\mid S) = 0.8\\)\n\n\\(P(\\overline{S}) = 0.7\\)\n\n\\(P(T \\mid \\overline{S}) = 0.6\\)\n\\(P(\\overline{T} \\mid \\overline{S}) = 0.4\\)"
  },
  {
    "objectID": "content/01_journal/01_probability.html#answer",
    "href": "content/01_journal/01_probability.html#answer",
    "title": "Probability Theory",
    "section": "",
    "text": "\\[\n\\begin{split}\nP(T \\cap S) = P(S) * P(T \\mid S) =0.3*0.2=0.06,\\\\\nP(T \\cap \\overline{S}) = P(\\overline{S}) * P(T \\mid \\overline{S}) =0.7*0.6=0.42,\\\\\nP(\\overline{T} \\cap S) = P(S) * P(\\overline{T} \\mid S) =0.3*0.8=0.24,\\\\\nP(\\overline{T} \\cap \\overline{S}) = P(\\overline{S}) * P(\\overline{T} \\mid \\overline{S}) =0.7*0.4=0.28.\n\\end{split}\n\\]\n\n\n\n\\[\nP(T \\cap S ) + P(T \\cap \\overline{S}) + P(\\overline{T} \\cap S) + P(\\overline{T} \\cap \\overline{S})\n= 1\n\\]"
  },
  {
    "objectID": "content/01_journal/02_statistics.html",
    "href": "content/01_journal/02_statistics.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Task 1\n\n\n\nVariable\nAge\nIncome\n\n\n\n\nExpected value\n33.5\n3510.7\n\n\nVariance\n340.6\n8.6256458^{6}\n\n\nStandard deviation\n18.5\n2936.9\n\n\n\n\n\nTask 2\nDepending on the goal it might make sense to compare both standard deviation relative to their mean, also known as the Coefficient of Variation (CV). This is possible as both variables are absolute and it might inform us of possible data analysis steps we can take. But, in general, it does not really make sense to compare the standard deviation of both distributions directly as they do not have the same unit, i.e. they represent completely different variables.\n\n\nTask 3\n\nCovariance: 2.97001^{4}\nCorrelation: 0.55\n\n\n\nTask 4\nThe pearson correlation coefficient is much easier to interpret, as it is a dimensionless value that has a predefined interval \\([-1,1]\\). This makes it possible to compare the strength of different relationships via the correlation coefficient.\nNote: The pearson correlation coefficient is sensitive to outlier and is best used with the assumption of normality. Other correlations, such as Spearman or Kendall, are more robust when the data is not normaly distributed.\n\n\nTask 5\n\n\n\nAge group\n[0,18]\n[18,65[\n[65,\\(\\infty\\)[\n\n\n\n\n\\(E[income]\\)\n391.22\n4743.23\n1604.19"
  },
  {
    "objectID": "content/01_journal/04_causality.html",
    "href": "content/01_journal/04_causality.html",
    "title": "Causality",
    "section": "",
    "text": "Below you can see the correlation between the import value of tomatoes in Germany and the number of cosmetic surgeries world wide.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you can imagine their is no causal link between the two. Nerveless, there is a significant correlation between both variables. The Pearson correlation coefficient is \\(R = 0.80\\). Both samples are normaly distributed."
  },
  {
    "objectID": "content/01_journal/04_causality.html#correlation",
    "href": "content/01_journal/04_causality.html#correlation",
    "title": "Causality",
    "section": "",
    "text": "As you can imagine their is no causal link between the two. Nerveless, there is a significant correlation between both variables. The Pearson correlation coefficient is \\(R = 0.80\\). Both samples are normaly distributed."
  },
  {
    "objectID": "content/01_journal/06_rct.html",
    "href": "content/01_journal/06_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/08_did.html",
    "href": "content/01_journal/08_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/10_rdd.html",
    "href": "content/01_journal/10_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  }
]